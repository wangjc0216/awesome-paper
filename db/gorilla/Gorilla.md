# Gorilla 


## 背景及目的

[Prometheus Storage v2](https://github.com/prometheus-junkyard/tsdb) 是基于Gorilla paper来实现的。设计的相关思路可参考[Writing a Time Series Database from Scratch](https://fabxc.org/tsdb/).
所以阅读Gorilla对于了解Prometheus Storage 的实现是非常重要的。

互联网时代，一家公司会部署大量服务，要保证他们的高可用，需要对它们进行7*24小时的监控。 给他们提供监控通常需要千万/s的测量。
一个有效的办法就是将这些测量(measurements)的存储和查询放在TSDB中。

## 概述

TSDB设计中的一个关键挑战就是如何 在效率、可扩展性、可靠性(efficiency, scalability, and reliability.)保持平衡。

**在文中，我们介绍Gorilla，memory TSDB。**

我们不需要关心单点，而是强调趋势(聚合分析，aggregate analysis)，最近的数据的价值是高于老的数据的，新数据对于更快监测和诊断持续问题的根本原因是更有价值的。

Gorilla在高性能读/写方面做了优化。

写优化方向： **会在面对失败时，在写入路径下可能丢失少量数据作为代价。**

读优化方向： 积极利用压缩技术( delta-of-delta timestamps /   XOR’d floating point), 可将数据保存在内存中。相比Hbase， 存储空间减小了10x，查询延迟减少73x。


## 介绍(1)

大规模互联网服务的目标是即使在出现以外故障的情况下，也能为用户保持高可用性和响应性，随着这些服务发展到全球，服务已经运行在数千台机器上了。

运营这些大规模服务的一个重要要求是准确监测底层系统的健康状态和性能，并在出现问题时快速识别和诊断它们。

- 写主导。数百个服务暴露的数据项，写入速率很容易超过千万条数据每秒点数。读取的速率通常比写入速率低几个量级。

- 状态转化。 通过服务升级、配置更新、网络中断导致的重大状态转换问题。**希望TSDB可以在较短的时间窗口内获取细粒度的聚合结果**。在十几秒/几十秒内发现状态转换是非常重要的。我们可以后续可以通过自动化的方式在问题广泛传播前修复问题。
```
  Thus, we wish for our TSDB to support fine-grained aggregations over short-time windows
 ```

- 高可用。网络分区或不同数据中心断开，在给定的数据中心内操作的系统应该也能够将数据写入到本地。

- 故障容忍度。将所有写入复制到多个区域，以免出现单点故障。

**Gorilla设计的insight是 监控系统的用户不会关心单个数据点，而是强调聚合分析。系统不存储用户信息，ACID不是TSDB的核心要求。 新的数据点比旧的数据点更有价值(更能及时发现问题)
Gorilla优化即使在遇到故障时也保持写入和读取的高可用性，代价是会丢掉少量数据。**

挑战来自高数据插入率、总数据量、实时聚合和可靠性要求。同时监控数据库85%的查询来自近26小时内收集的数据。进一步的分析使得我们确认我们使用内存数据库替换基于磁盘的数据库。
更长远来看，可以将Gorilla这个内存数据库看做是基于磁盘持久存储的cache。我们可以获得内存系统的速度，也可以获得磁盘的持久性。

2015年，Facebook就存储了20亿个time series，其中每秒约添加1200万个sample/point,这代表每天超过1万亿点，每个sample/point 占16个bytes，那么存储就会占16TB.
我们使用XOR浮点压缩技术，会压缩到平均每个sample/point到1.37bytes,缩小了12x。

我们在不同的数据中心部署多个Gorilla实例，并将数据流式传输到每一个实例来满足可用性要求，而不试图保证一致性(consistency)。


## 背景和要求(2)




## TSDB之间的比较(3)  / 与之前的方案进行比对




## 实现原理/数据模型(4)




## Gorilla的新工具(5)



## 经验(experience) (6)



## 未来工作开展(7)



## 结论(8)





